{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0614425c-7f0f-4ecb-b8dc-f4c596488563",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Building Solutions with LLMs and Retrieval Augmented Generation (RAG): Introduction\n",
    "\n",
    "Welcome to this AI workshop! In this workshop we will get deeper into AI, RAG, chatbots, embeddings, evaluations and more.\n",
    "But first lets get used to this notebook enviroment.\n",
    "\n",
    "### Getting started with Jupyter Notebooks\n",
    "Notebooks are essential tools in the toolkit of data scientists and machine learning engineers. \n",
    "They facilitate interactive coding and effective visualization of results, making them invaluable for data exploration and analysis.\n",
    "Jupyter allows you to create cells that can contain either text or code. \n",
    "You can execute code cells by pressing Ctrl + Enter, enabling you to run your code and see results instantly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "1. Run the cell bellow a couple of times to get used to the notebook behaviour.\n",
    "2. Restart the kernel (Restart) button above and run again this code to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a if it does not exist\n",
    "if  'a' not in globals():\n",
    "    a = 42\n",
    "\n",
    "a = a+1\n",
    "\n",
    "# press ctrl+enter to run the cell, do that multiple times\n",
    "# notice how the variable `a` is persistent in the cell\n",
    "# you can run full programs in a notebook.\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## LLMs\n",
    "LLMs, or Large Language Models, are advanced AI systems designed to understand and generate human language. They use deep learning techniques and are trained on vast amounts of text data. Key features include their large scale (billions of parameters), ability to perform various language tasks (like translation and summarization), and contextual understanding. \n",
    "### MistralAI\n",
    "\n",
    "For this workshop we wil use [Mistral AI](https://mistral.ai/) langauge models, which are similar in concept to OpenAI's ChatGPT and Anthropic's Claude. Mistral AI offers both premier models and free models, with a strong emphasis on open-source availability. We are going to use [Mistral Small](https://docs.mistral.ai/getting-started/models/models_overview/#premier-models) model today. \n",
    "\n",
    "If your installation finalized correctly you should have mistral already installed. \n",
    "The command below checks if mistral is installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mistralai\n",
      "Version: 1.1.0\n",
      "Summary: Python Client SDK for the Mistral AI API.\n",
      "Home-page: https://github.com/mistralai/client-python.git\n",
      "Author: Mistral\n",
      "Author-email: \n",
      "License: \n",
      "Location: /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages\n",
      "Requires: eval-type-backport, httpx, jsonpath-python, pydantic, python-dateutil, typing-inspect\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for some reason mistral did not installed successfully you can get it (and all other dependencies of this workshop by installing it directly with the command below), just uncomment the lines which start with \"!pip install\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/explodinggradients/ragas (from -r ../requirements.txt (line 10))\n",
      "  Cloning https://github.com/explodinggradients/ragas to /tmp/pip-req-build-j98sh9fl\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/explodinggradients/ragas /tmp/pip-req-build-j98sh9fl\n",
      "  Resolved https://github.com/explodinggradients/ragas to commit cc31f65d4b7c7cd6bbf686b9073a0dfaacfbcbc5\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sentence-transformers~=3.2.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: pandas~=2.2.3 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: mistralai~=1.1.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: watchdog~=5.0.3 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 4)) (5.0.3)\n",
      "Requirement already satisfied: streamlit~=1.39.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 5)) (1.39.0)\n",
      "Requirement already satisfied: python-dotenv in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 6)) (1.0.1)\n",
      "Requirement already satisfied: ipykernel in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 7)) (6.29.5)\n",
      "Requirement already satisfied: fire in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: langchain_mistralai in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 9)) (0.2.2)\n",
      "Requirement already satisfied: langchain-huggingface in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 11)) (0.1.2)\n",
      "Requirement already satisfied: nltk in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 12)) (3.9.1)\n",
      "Requirement already satisfied: rapidfuzz in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 13)) (3.10.1)\n",
      "Requirement already satisfied: matplotlib in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 14)) (3.9.2)\n",
      "Requirement already satisfied: einops in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 15)) (0.8.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (4.46.3)\n",
      "Requirement already satisfied: tqdm in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (1.5.2)\n",
      "Requirement already satisfied: scipy in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (10.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from pandas~=2.2.3->-r ../requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from pandas~=2.2.3->-r ../requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from pandas~=2.2.3->-r ../requirements.txt (line 2)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from pandas~=2.2.3->-r ../requirements.txt (line 2)) (2024.2)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from mistralai~=1.1.0->-r ../requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from mistralai~=1.1.0->-r ../requirements.txt (line 3)) (0.27.2)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from mistralai~=1.1.0->-r ../requirements.txt (line 3)) (1.0.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from mistralai~=1.1.0->-r ../requirements.txt (line 3)) (2.10.2)\n",
      "Requirement already satisfied: typing-inspect<0.10.0,>=0.9.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from mistralai~=1.1.0->-r ../requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas~=2.2.3->-r ../requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (8.1.7)\n",
      "Requirement already satisfied: packaging<25,>=20 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (24.2)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (5.28.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (18.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (4.12.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 5)) (6.4.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (1.8.9)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (8.29.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (1.6.0)\n",
      "Requirement already satisfied: psutil in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (6.1.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (26.2.0)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 7)) (5.14.3)\n",
      "Requirement already satisfied: termcolor in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from fire->-r ../requirements.txt (line 8)) (2.5.0)\n",
      "Requirement already satisfied: httpx-sse<1,>=0.3.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain_mistralai->-r ../requirements.txt (line 9)) (0.4.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain_mistralai->-r ../requirements.txt (line 9)) (0.3.21)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain_mistralai->-r ../requirements.txt (line 9)) (0.20.4)\n",
      "Requirement already satisfied: datasets in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (3.1.0)\n",
      "Requirement already satisfied: tiktoken in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (0.8.0)\n",
      "Requirement already satisfied: langchain in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (0.3.8)\n",
      "Requirement already satisfied: langchain-community in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (0.3.8)\n",
      "Requirement already satisfied: langchain_openai in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (0.2.10)\n",
      "Requirement already satisfied: appdirs in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (1.4.4)\n",
      "Requirement already satisfied: openai>1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (1.55.1)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (0.3.4)\n",
      "Requirement already satisfied: joblib in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from nltk->-r ../requirements.txt (line 12)) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from nltk->-r ../requirements.txt (line 12)) (2024.11.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from matplotlib->-r ../requirements.txt (line 14)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from matplotlib->-r ../requirements.txt (line 14)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from matplotlib->-r ../requirements.txt (line 14)) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from matplotlib->-r ../requirements.txt (line 14)) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from matplotlib->-r ../requirements.txt (line 14)) (3.2.0)\n",
      "Requirement already satisfied: jinja2 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 5)) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 5)) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 5)) (1.14.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit~=1.39.0->-r ../requirements.txt (line 5)) (4.0.11)\n",
      "Requirement already satisfied: anyio in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (1.0.7)\n",
      "Requirement already satisfied: idna in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: sniffio in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (0.14.0)\n",
      "Requirement already satisfied: filelock in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (2024.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: decorator in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (0.19.2)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r ../requirements.txt (line 7)) (4.3.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 9)) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 9)) (0.1.146)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from openai>1->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from openai>1->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (0.7.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit~=1.39.0->-r ../requirements.txt (line 5)) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit~=1.39.0->-r ../requirements.txt (line 5)) (2.2.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit~=1.39.0->-r ../requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: networkx in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (0.4.5)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from typing-inspect<0.10.0,>=0.9.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (3.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (2.0.35)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (0.3.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain-community->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain-community->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (2.6.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from scikit-learn->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (1.18.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (3.23.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit~=1.39.0->-r ../requirements.txt (line 5)) (5.0.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from jinja2->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 9)) (3.0.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 5)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 5)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 5)) (0.21.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 9)) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 9)) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit~=1.39.0->-r ../requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (0.2.13)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas==0.2.7.dev2+gcc31f65->-r ../requirements.txt (line 10)) (3.1.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 7)) (0.2.3)\n",
      "Obtaining file:///workspaces/ai-rag-quiz-workshop\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: chat_solution\n",
      "  Building editable for chat_solution (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for chat_solution: filename=chat_solution-1.0.0-0.editable-py3-none-any.whl size=2798 sha256=161ad68e37699e0e411f6e9c8091877ea110c239f7c0df68903438d8fe037043\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jj6_h6m3/wheels/a4/c6/d9/ae4576476be94afa1f5b82e4417e5583256c74d55dc9e6c651\n",
      "Successfully built chat_solution\n",
      "Installing collected packages: chat_solution\n",
      "  Attempting uninstall: chat_solution\n",
      "    Found existing installation: chat_solution 1.0.0\n",
      "    Uninstalling chat_solution-1.0.0:\n",
      "      Successfully uninstalled chat_solution-1.0.0\n",
      "Successfully installed chat_solution-1.0.0\n"
     ]
    }
   ],
   "source": [
    "# this command will install all the dependencies of the requirements.txt file\n",
    "!pip install -r ../requirements.txt\n",
    "\n",
    "# this command will install the code of the project itsel\n",
    "!pip install -e ../\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting access to Mistral LLMs\n",
    "\n",
    "LLMs are large in size and use a lot of computing resources. The most common way to use them is to run them in the cloud via network calls. For the sake of this workshop, we will use the cloud version as well, so we don't need to download large models. If you are interested in running LLMs locally, check out [ollama](https://github.com/ollama/ollama), arguably the most advanced project that does this.\n",
    "\n",
    "\n",
    "#### API key\n",
    "The second step is to get a Mistral API key. You can find some API keys we prepared for this workshop in this [sheet](https://docs.google.com/spreadsheets/d/1ZwTpkG6OOuVrOx8nzPmgai_7Hwpo8Kun7yZrOmg_5K4/edit?gid=0#gid=0). Get the key (please write your name next to it in the sheet so that people know it is taken) and write it to the .env file using the command below in Task 2.\n",
    "\n",
    "If you want to get your own API key, you can do it on the [website here](https://auth.mistral.ai/ui/registration).You will need to sign up using your email address and phone number and create a new API key [here]((https://console.mistral.ai/api-keys/)). If you are having any trouble, check out the video instructions on how to do it [here](https://drive.google.com/file/d/1mqwkX1BRvg_RMZJQHjtvKq31RjWZ9MdK/view)\n",
    "\n",
    "\n",
    "# Task 2\n",
    "\n",
    "Use the cell below to write your API key into the file so you can use Mistral. We write the variable into a file named .env. The .env files are a standard in Python to load information like API keys and passwords that should not stay with the code for security reasons. **Make sure to delete your key from the cell once you write it to the .env file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file .env\n",
    "\n",
    "mistral_api_key = 'nnxPUMAD1eUJdJOVr9Futb7ObWAMlauU'\n",
    "with open('../.env', 'w') as f:\n",
    "    f.write(f'MISTRAL_API_KEY=\"{mistral_api_key}\"')\n",
    "\n",
    "# make sure to delete the API key from this cell before commiting the file so you dont save your key in the repo which is a security risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if it worked\n",
    "\n",
    "Run the command below to check if writing the key worked.  It will trigger an error if it did not.  You might need to restart the kernel if it does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables from /workspaces/ai-rag-quiz-workshop/.env\n",
      "If you reached this point is because it wrote a key in the right place :)\n"
     ]
    }
   ],
   "source": [
    "# we now reload the the configuration file and it should show your key\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "env_file = find_dotenv()\n",
    "print(f\"Loading environment variables from {env_file}\")\n",
    "load_dotenv(env_file)\n",
    "\n",
    "import os\n",
    "env = os.getenv('MISTRAL_API_KEY')\n",
    "\n",
    "if not env:\n",
    "    raise ValueError(\"The API key is not set. Please set the MISTRAL_API_KEY environment variable.\")\n",
    "if env == 'REPLACE WITH YOUR KEY':\n",
    "    raise ValueError(\"You did not repalce the value with the real key\")\n",
    "print(\"If you reached this point is because it wrote a key in the right place :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running mistral\n",
    "Let's run the code below to import Mistral and initialize the Mistral client: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I don't have a physical age as I'm a text-based AI model. I was created in 2021. How about you? How old are you?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Retrieve the Mistral API key from the environment variables\n",
    "mistral_api_key = os.getenv('MISTRAL_API_KEY')\n",
    "\n",
    "# Initialize the Mistral client with the API key\n",
    "mistral_client = Mistral(api_key=mistral_api_key)\n",
    "\n",
    "# The model below is the specific model we want to use\n",
    "model_name = \"mistral-small-latest\"\n",
    "\n",
    "# The code below defines a function `call_mistral_model` that sends a message to a Mistral model and returns the model's response text.\n",
    "response = mistral_client.chat.complete(\n",
    "    model = model_name,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"hello! How old are you?\",\n",
    "        }\n",
    "        ]\n",
    "    )\n",
    "# Extract only the text from the response\n",
    "response_text = response.choices[0].message.content\n",
    "print(response_text)\n",
    "\n",
    "## not how the result resembles natural language. Its the exact same concept as chatgpt.\n",
    "## feel free to play around with the prompt and see how the results change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having all of this complex code to call the LLM, we can simplify it by moving this complexity to a Python script. From the code below onwards, we will replace our calls to the Mistral API with calls to the LargeLanguageModel class.\n",
    "\n",
    "Try it out and take a look at the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables from /workspaces/ai-rag-quiz-workshop/.env\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSDKError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/workspaces/ai-rag-quiz-workshop/chat_solution/llm.py:24\u001b[0m, in \u001b[0;36mLargeLanguageModel.call\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m chat_response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages/mistralai/chat.py:138\u001b[0m, in \u001b[0;36mChat.complete\u001b[0;34m(self, model, messages, temperature, top_p, max_tokens, min_tokens, stream, stop, random_seed, response_format, tools, tool_choice, safe_prompt, retries, server_url, timeout_ms)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mmatch_response(http_res, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5XX\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m models\u001b[38;5;241m.\u001b[39mSDKError(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI error occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m, http_res\u001b[38;5;241m.\u001b[39mstatus_code, http_res\u001b[38;5;241m.\u001b[39mtext, http_res\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    142\u001b[0m content_type \u001b[38;5;241m=\u001b[39m http_res\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mSDKError\u001b[0m: API error occurred: Status 429\n{\"message\":\"Requests rate limit exceeded\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m llm \u001b[38;5;241m=\u001b[39m LargeLanguageModel()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Make a call to Mistral using the LargeLanguageModel class\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello! What is your name?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m/workspaces/ai-rag-quiz-workshop/chat_solution/llm.py:42\u001b[0m, in \u001b[0;36mLargeLanguageModel.call\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     40\u001b[0m     time_to_wait \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m attempt\n\u001b[1;32m     41\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate limit error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting \u001b[39m\u001b[38;5;132;01m{time_to_wait}\u001b[39;00m\u001b[38;5;124m seconds before retrying\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_to_wait\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApi key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from chat_solution.llm import LargeLanguageModel\n",
    "\n",
    "# Initialize an instance of the LargeLanguageModel class\n",
    "llm = LargeLanguageModel()\n",
    "# Make a call to Mistral using the LargeLanguageModel class\n",
    "response = llm.call(\"hello! What is your name?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond calling the LLM, the class contains rate limiting handling logic. Rate limiting is a mechanism implemented by APIs to control the number of requests a user can make in a given time frame. This is done to prevent abuse so that one user doesn't consume the entire capacity and slow down the service. However, rate limiting can be annoying because it can interrupt your workflow and force you to wait before making more requests.\n",
    "\n",
    "To make things easier, we've implemented a rate limit error controller in the LargeLanguageModel class (see usage example below) that automatically adds sleep intervals between requests to avoid exceeding the rate limit. We will use the LargeLanguageModel class moving forward to make calls to Mistral AI. This class has the same logic as the examples we showed above but includes a mechanism to counter the rate limiting issue.    \n",
    "\n",
    "Have a look at the class by Ctrl+clicking on the LargeLanguageModel name to jump directly to its implementation.\n",
    "\n",
    "## Exploring the LLM\n",
    "Now that we have seen how to make a basic call to the Mistral model using the [LargeLanguageModel](../chat_solution/llm.py) class, let's try some more prompts to see how the model responds to different types of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Modifying Prompts\n",
    "Below are some example use cases of how to use an LLM such as Mistral. Play around with the prompts and see the results. Modify the prompts to see how the model's responses change. This will help you understand how to craft effective prompts and get the desired output from the model.\n",
    "\n",
    "Try to:\n",
    "- Ask different types of questions\n",
    "- Change the text for summarization or extraction (see examples 2 and 3 below)\n",
    "- Alter the style of the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Asking for Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! 42Berlin is a coding school that follows the innovative educational model pioneered by 42 Network, which originated in France. Here are some key aspects of 42Berlin:\n",
      "\n",
      "### Educational Model\n",
      "1. **Peer-to-Peer Learning**: 42Berlin emphasizes a peer-to-peer learning environment where students learn from and teach each other. This model encourages collaboration and mutual support.\n",
      "\n",
      "2. **Project-Based Learning**: The curriculum is heavily focused on practical projects. Students work on real-world coding challenges and develop software projects from start to finish.\n",
      "\n",
      "3. **No Teachers or Classes**: Unlike traditional schools, 42Berlin does not have teachers or formal classes. Instead, students learn through hands-on projects, online resources, and mentorship from more experienced peers.\n",
      "\n",
      "### Admission Process\n",
      "1. **Piscine (Pool)**: The admission process begins with a four-week intensive coding bootcamp called the \"Piscine.\" During this period, applicants are evaluated on their problem-solving skills, motivation, and ability to work in a team.\n",
      "\n",
      "2. **No Prerequisites**: 42Berlin does not require any formal education or prior coding experience. The focus is on the candidate's aptitude and motivation to learn.\n",
      "\n",
      "### Curriculum\n",
      "1. **Full-Stack Development**: The curriculum covers a wide range of topics, including web development, mobile development, data science, and more. Students gain experience with various programming languages and technologies.\n",
      "\n",
      "2. **Soft Skills**: In addition to technical skills, the program emphasizes the development of soft skills such as communication, teamwork, and project management.\n",
      "\n",
      "### Duration and Structure\n",
      "1. **Flexible Schedule**: Students can work at their own pace, and there is no fixed schedule. This allows for a high degree of flexibility and self-directed learning.\n",
      "\n",
      "2. **Duration**: The program typically takes around 3 to 5 years to complete, depending on the student's pace and dedication.\n",
      "\n",
      "### Community and Network\n",
      "1. **Global Network**: As part of the 42 Network, 42Berlin students have access to a global community of coders and alumni. This network can be invaluable for career opportunities and ongoing learning.\n",
      "\n",
      "2. **Events and Workshops**: The school hosts various events, workshops, and hackathons to keep students engaged and connected with the tech industry.\n",
      "\n",
      "### Career Opportunities\n",
      "1. **Job Placement**: 42Berlin has strong ties with the tech industry, and many graduates go on to secure jobs in software development, startups, and tech companies.\n",
      "\n",
      "2. **Entrepreneurship**: The program also encourages entrepreneurship, and some graduates go on to start their own tech ventures.\n",
      "\n",
      "### Cost\n",
      "1. **Tuition-Free**: 42Berlin is tuition-free, which makes it an attractive option for those looking to break into the tech industry without incurring significant debt.\n",
      "\n",
      "Overall, 42Berlin offers a unique and innovative approach to coding education, focusing on practical skills, peer-to-peer learning, and real-world projects. It is an excellent option for motivated individuals looking to pursue a career in software development.\n"
     ]
    }
   ],
   "source": [
    "# Example: Asking for information\n",
    "prompt = \"Can you tell me about coding school 42Berlin?\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Summarizing a Given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42Berlin is a tuition-free coding school empowering the next generation of coders through accessible and inclusive tech education.\n"
     ]
    }
   ],
   "source": [
    "# Change the text into something else to see the results\n",
    "text_to_summarize = (\n",
    "    \"\"\"\n",
    "    42Berlin is a non-profit coding school offering software engineering education completely tuition free. \n",
    "    By making tech education more accessible and inclusive, they empower the next generation of coders.\n",
    "    Founded in 2021 and based in central Neukölln, we train our students up to the equivalent of Master’s level \n",
    "    and implement peer-learning methodologies that give autonomy to each student.\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "prompt = f\"Summarize the following text in one brief sentence: {text_to_summarize}\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: Extracting Information from a Given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text, there is no mention of the year 42Berlin was founded. The text only describes the nature and mission of 42Berlin without providing any specific founding date.\n"
     ]
    }
   ],
   "source": [
    "# Change the text into something else to see the results\n",
    "text_to_extract_from = (\n",
    "    \"\"\"\n",
    "    42Berlin is a non-profit coding school offering software engineering education completely tuition free. \n",
    "    By making tech education more accessible and inclusive, they empower the next generation of coders.\n",
    "    We train our students up to the equivalent of Master’s level \n",
    "    and implement peer-learning methodologies that give autonomy to each student.\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "prompt = f\"Extract the year 42Berlin was founded from the following text: {text_to_extract_from}. Don't lie.\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 3: Doing Maths -> When will the LLM make a mistake?\n",
    "\n",
    "LLMs are trained on text language so they are not necessarily good with maths. We can be confident that it will fail to produce the correct result if you ask a question that is complex enough. On the notebook below we increase the complexity of the task at every run. Run it until you see it diverging.\n",
    "\n",
    "What was the number of iterations necessary to make it break?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct result:  827240261886336764177\n",
      "Mistral result:  The result of 17 raised to the power of 17 (17^17) is a very large number. Here it is:\n",
      "\n",
      "17^17 = 23,298,081,803,392,982,912,997,984,175,577,600,682,230,208\n",
      "\n",
      "This number has 45 digits.\n"
     ]
    }
   ],
   "source": [
    "if 'a' not in globals() or 'b' not in globals():\n",
    "    a = 1\n",
    "    b = 1\n",
    "a = a + 1\n",
    "b = b + 1\n",
    "\n",
    "## ** in python is the power operator meaning: a to the power of b\n",
    "print(\"Correct result: \", a**b)\n",
    "response = llm.call(f\"what is the results of {a} ** {b}\")\n",
    "print(\"Mistral result: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably noted, mistral does not only return the results, it also explains it step by step. For example:\n",
    "\n",
    "```\n",
    "The result of 4 raised to the power of 4 (4 ** 4) is calculated as:\n",
    "4 * 4 * 4 * 4 = 256\n",
    "```\n",
    "\n",
    "Mistral and other language models deliberatelly add this longer explanations as they help the model commit less mistakes. This pattern is also known as chain of thought reasonsing. And is one of the most robust ways to improve LLM's responses. When you write prompts think about how to explain the steps of the reasonsing to improve the performance of your propmts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucination\n",
    "LLMs sometimes generate responses that are plausible-sounding but factually incorrect or nonsensical. This phenomenon is known as \"hallucination\".\n",
    "<br><br>\n",
    "***Hallucination*** can occur because the model generates text based on patterns in the training data rather than actual knowledge or retrieval of relevant information.\n",
    "LLMs will produce the most likelly words that they would find in similar text, they have no clue about fact vs fiction. They can confidently produce writing about things they have no clue about.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Demonstrating Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will ask the model a question that it might to hallucinate an answer for, showing the limitations of relying solely on language generation without retrieval.\n",
    "\n",
    "Try running the command below a few times in a row and see how the response by the LLM changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response likely to hallucinate:\n",
      "\n",
      "The workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech was named \"MLOps for Women.\" This workshop aimed to empower women in the field of machine learning operations (MLOps) by providing them with practical skills, knowledge, and networking opportunities.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question likely to cause hallucination\n",
    "prompt = \"What was the name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech?\"\n",
    "response = llm.call(prompt)\n",
    "print(\"Response likely to hallucinate:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move on to the next section on Retrieval-Augmented Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique that incorporates external information to a LLM to generate better responses. \n",
    "\n",
    "In RAG, a retrieval component searches and retrieves relevant information from a knowledge base or external documents, and a generation component uses this information to generate responses.\n",
    "This approach allows the model to access up-to-date information and provide more detailed and accurate answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Simple RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will demonstrate a simple example of how to use Retrieval-Augmented Generation. We will use a predefined set of documents, retrieve relevant information based on a query, and then generate a response using the retrieved information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_prompt(message: str, context: str):\n",
    "    \"\"\"\n",
    "    Message is the question that the user is asking.\n",
    "    Context is the information that we want to use to answer the question.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Answer the question only using the provided content.\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        User Question: {message}\n",
    "\n",
    "        Be helpful and friendly. If the information cannot be found respond with \"I don't know\"\n",
    "        \"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the code in the cell below, you can compare how our LLM responses differ by the information that you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " The workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech was named \"MLOps for Women.\" This workshop aimed to empower women in the field of machine learning operations (MLOps) by providing them with practical skills, knowledge, and networking opportunities.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " The name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech is \"AI Launchpad - Building Your First ML Pipeline.\"\n"
     ]
    }
   ],
   "source": [
    "message = \"What was the name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech?\"\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "# The workshop the MLOps Community hosted together with Girls in Tech Germany was called \"AI Launchpad: Building Your First Ml Pipeline\" or simply \"Building Your First ML Pipeline\"\n",
    "# We copy paste the info from our Eventbrite event page from the previous workshop and use this as context for the model to retrieve the right info from\n",
    "context = \"\"\"Title: AI Launchpad - Building Your First ML Pipeline: \n",
    "On Wednesday, June 5th, 2024, the MLOps Community Berlin in collaboration with Girls in Tech Germany hosted an interactive workshop for beginners who want to kick start their career in AI/ML. \n",
    "The workshop starts at 18.00h at 42Berlin. \n",
    "\n",
    "🔍 Why Attend?\n",
    "\n",
    "Gain hands-on experience building your first ML pipeline in an agile way\n",
    "Apply the fundamentals of statistical modeling and basic Python\n",
    "Opportunities to improve your portfolio \n",
    "Connect with ML professionals at different levels of seniority\n",
    "\n",
    "\n",
    "✨ The Agenda: \n",
    "\n",
    "6:00 pm - Arrive & Pizza \n",
    "6:30 pm -  Introduction MLOps and GiT\n",
    "6:45 pm - Workshop Introduction\n",
    "7:30 pm - Break\n",
    "7:45 pm - Workshop\n",
    "9:45 pm - Networking\n",
    "\n",
    "\n",
    "🎉 Highlights:\n",
    "\n",
    "Food and drinks provided\n",
    "Engaging discussions and networking opportunities\n",
    "Bring your laptop and get ready to learn!\n",
    "\n",
    "\n",
    "💼 Who Should Attend?\n",
    "\n",
    "Individuals starting their career in Machine Learning or Artificial Intelligence\n",
    "Those looking to transition into the field of AI/ML\n",
    "Anyone interested in contributing to and learning from the ML community\n",
    "Don't miss out on this chance to gain practical AI/ML skills while expanding your professional network! \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " I'm an assistant that operates solely on the data it has been trained on up until 2021, and I don't have real-time or future weather data. Therefore, I can't provide the weather forecast for Berlin on the 10th of December, 2027. For the most accurate and up-to-date weather information, I recommend checking a reliable weather website or application closer to the date.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " On the 10th of December, 2027, the weather in Berlin is expected to be around 10 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "# Let's try the same thing with Berlin weather data!\n",
    "context = \"\"\"\n",
    "The weather in Berlin  December of 2027 will be around 13 degrees Celsius.\n",
    "Specific dates:\n",
    "- 10th of December: 10 degrees Celsius\n",
    "- 15th of December: 15 degrees Celsius\n",
    "- 20th of December: 7 degrees Celsius\n",
    "\"\"\"\n",
    "\n",
    "message = \"What will be the weather in Berlin on the 10th of December of 2027?\"\n",
    "\n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "Now try it yourself! Can you find some content on the internet (think, for example, recent news articles or very specific, locally relevant information that the LLM normally would not have access to). \n",
    "\n",
    "Play around with it and let the creative juices flow. Can you discover some more use cases for which you can use RAG can help make our LLM smarter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "YOUR CONTEXT HERE\n",
    "\"\"\" \n",
    "\n",
    "message = \"\"\"\n",
    "YOUR MESSAGE HERE\n",
    "\"\"\" \n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "If you want to make your prompting even more advanced, you can look into Prompt Engineering best practices. <br><br>\n",
    "***Prompt Engineering*** is the process of designing and refining LLM prompts. It involves crafting questions, instructions, or statements that guide the model to produce desired outputs. Effective prompt engineering can significantly enhance the quality, relevance, and accuracy of the responses generated by the model.  \n",
    "\n",
    "Best practices change over time and are somewhat different depending on the LLM type, but general guidelines are:\n",
    "\n",
    "* **Clarity and Specificity:** Clearly state what you want the model to do. Avoid ambiguous language.\n",
    "* **Context and Detail:** Provide sufficient context to guide the model.\n",
    "* **Iterative Refinement:** Experiment with different phrasings and structures to see what works best.\n",
    "* **Constraints and Instructions:** Specify any constraints or formats you want the response to follow.\n",
    "* **Examples and Templates:** Use examples to show the model what kind of response you expect.\n",
    "* **Step-by-Step Reasoning:** Instead of providing a direct answer, aske the model to explain its thought process step-by-step. \n",
    "* **Breaking Down Complex Tasks:** For tasks that require multiple stages of reasoning or involve complex information, breaking them down into smaller, separated steps can lead to better results.\n",
    "\n",
    "You can learn more about prompt engineering [here](https://www.promptingguide.ai/). \n",
    "Some very useful examples of best practices for OpenAI GPT models can be found [here](https://platform.openai.com/docs/guides/prompt-engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it! \n",
    "\n",
    "RAGs enrich the prompt with additional information about the topic to generate responses. The external information can come from various sources, such as PDFs, Google search results, social media posts, and more. With that, we’ve built a simple Q&A RAG.\n",
    "\n",
    "## ===> Now head to notebook 2 on embedddings and how llms undertand language\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1848951197487297,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Steven Test Playground",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".virtualenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
