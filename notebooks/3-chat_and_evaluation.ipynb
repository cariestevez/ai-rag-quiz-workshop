{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e824fd",
   "metadata": {},
   "source": [
    "\n",
    "# Building our AI Quiz and evaluating its performance\n",
    "\n",
    "Welcome to the last notebook of this workshop content we will walk you through how to build our chat web application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4fc601",
   "metadata": {},
   "source": [
    "Now lets jump to our application. The purpose of this part is to give you an overview of everything you need to do to get an chat-application working.\n",
    "\n",
    "The folder chat_solution contains the app. \n",
    "\n",
    "The most important files are:\n",
    "\n",
    "- create_db.py: This file contians the document / embedding logic\n",
    "- rag.py: the logic of how call the llm with documents\n",
    "- start_streamlit.py: where our program starts, contains the ui logic and the calls to the main components\n",
    "\n",
    "\n",
    "To use our chat we first need to make sure we have documents stored in the database. Lets do it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c107c1db-f471-4ccc-8fad-e6fbe08d0eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables from /workspaces/ai-rag-quiz-workshop/.env\n",
      "Created 74 chunks of size 700 with overlap 200\n",
      "Documents added to the database successfully\n",
      "['resource-intensive.\\nClosed-Source LLMs\\nClosed-source LLMs are developed and maintained by private companies or organizations, with the source code, training data, and model architecture kept proprietary. Access to these models is typically provided through APIs or licensed software, often involving subscription fees or pay-per-use pricing models. These models come with professional support, regular updates, and maintenance provided by the developers. Examples of closed-source LLMs include GPT-4, Claude, and Megatron-Turing NLG.\\nAdvantages:\\nClosed-source models are often highly optimized for performance and accuracy, providing superior results.\\nThey come with access to professional support an', 'resource-intensive.\\nClosed-Source LLMs\\nClosed-source LLMs are developed and maintained by private companies or organizations, with the source code, training data, and model architecture kept proprietary. Access to these models is typically provided through APIs or licensed software, often involving subscription fees or pay-per-use pricing models. These models come with professional support, regular updates, and maintenance provided by the developers. Examples of closed-source LLMs include GPT-4, Claude, and Megatron-Turing NLG.\\nAdvantages:\\nClosed-source models are often highly optimized for performance and accuracy, providing superior results.\\nThey come with access to professional support an', 'resource-intensive.\\nClosed-Source LLMs\\nClosed-source LLMs are developed and maintained by private companies or organizations, with the source code, training data, and model architecture kept proprietary. Access to these models is typically provided through APIs or licensed software, often involving subscription fees or pay-per-use pricing models. These models come with professional support, regular updates, and maintenance provided by the developers. Examples of closed-source LLMs include GPT-4, Claude, and Megatron-Turing NLG.\\nAdvantages:\\nClosed-source models are often highly optimized for performance and accuracy, providing superior results.\\nThey come with access to professional support an', 'ity to process and generate text that resembles natural language, performing tasks related to natural language processing (NLP). However, LLMs stand out due to their significant size, characterized by two main factors:\\nLarge Training Datasets: LLMs are trained using vast amounts of data, allowing them to learn a wide range of language patterns and nuances.\\nHuge Number of Learnable Parameters: LLMs have a massive number of learnable parameters. These parameters represent the underlying structure of the training data and enable the models to perform tasks on new or never-before-seen data effectively.\\nThese characteristics make LLMs particularly powerful and versatile in handling complex langua', 'ity to process and generate text that resembles natural language, performing tasks related to natural language processing (NLP). However, LLMs stand out due to their significant size, characterized by two main factors:\\nLarge Training Datasets: LLMs are trained using vast amounts of data, allowing them to learn a wide range of language patterns and nuances.\\nHuge Number of Learnable Parameters: LLMs have a massive number of learnable parameters. These parameters represent the underlying structure of the training data and enable the models to perform tasks on new or never-before-seen data effectively.\\nThese characteristics make LLMs particularly powerful and versatile in handling complex langua']\n"
     ]
    }
   ],
   "source": [
    "from chat_solution.create_db import create_db\n",
    "\n",
    "db = create_db()\n",
    "print(db.retrieve(\"what is a llm?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088798a1",
   "metadata": {},
   "source": [
    "## Our RAG script\n",
    "\n",
    "The main part of this chat application is to create a rag call. The LearningAssistant in rag.py is where we implemented our main logic.\n",
    "Explore it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be0362f7-abc1-4043-9ba8-a3648696e8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Api key: wRmdESKVR409hMRjCtauf5Gqib0bgepw\n"
     ]
    },
    {
     "ename": "SDKError",
     "evalue": "API error occurred: Status 401\n{\n  \"message\":\"Unauthorized\",\n  \"request_id\":\"9ed2e21980562c3d3961ee778dd78503\"\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSDKError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m rag \u001b[38;5;241m=\u001b[39m LearningAssistant()  \n\u001b[1;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is an hallucination?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m/workspaces/ai-rag-quiz-workshop/chat_solution/rag.py:59\u001b[0m, in \u001b[0;36mLearningAssistant.query\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments_retrieved \u001b[38;5;241m=\u001b[39m documents\n\u001b[1;32m     58\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_prompt(documents, query)\n\u001b[0;32m---> 59\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation_history\u001b[38;5;241m.\u001b[39mappend((query, response))\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/workspaces/ai-rag-quiz-workshop/chat_solution/llm.py:45\u001b[0m, in \u001b[0;36mLargeLanguageModel.call\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApi key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate limit exceeded after retries\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/ai-rag-quiz-workshop/chat_solution/llm.py:24\u001b[0m, in \u001b[0;36mLargeLanguageModel.call\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m         chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m         response_text \u001b[38;5;241m=\u001b[39m chat_response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response_text\n",
      "File \u001b[0;32m/workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages/mistralai/chat.py:138\u001b[0m, in \u001b[0;36mChat.complete\u001b[0;34m(self, model, messages, temperature, top_p, max_tokens, min_tokens, stream, stop, random_seed, response_format, tools, tool_choice, safe_prompt, retries, server_url, timeout_ms)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m models\u001b[38;5;241m.\u001b[39mHTTPValidationError(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mmatch_response(http_res, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5XX\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m models\u001b[38;5;241m.\u001b[39mSDKError(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI error occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m, http_res\u001b[38;5;241m.\u001b[39mstatus_code, http_res\u001b[38;5;241m.\u001b[39mtext, http_res\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    142\u001b[0m content_type \u001b[38;5;241m=\u001b[39m http_res\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m models\u001b[38;5;241m.\u001b[39mSDKError(\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected response received (code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhttp_res\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    145\u001b[0m     http_res\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    146\u001b[0m     http_res\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m    147\u001b[0m     http_res,\n\u001b[1;32m    148\u001b[0m )\n",
      "\u001b[0;31mSDKError\u001b[0m: API error occurred: Status 401\n{\n  \"message\":\"Unauthorized\",\n  \"request_id\":\"9ed2e21980562c3d3961ee778dd78503\"\n}"
     ]
    }
   ],
   "source": [
    "# User input and response handling\n",
    "from chat_solution.rag import LearningAssistant\n",
    "\n",
    "rag = LearningAssistant()  \n",
    "query = \"what is an hallucination?\"\n",
    "response = rag.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7391ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now change teh instruc\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c4988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An hallucination? Oh, you mean when an AI starts seeing little green men and thinks it's on a spaceship? No, wait, that's just a regular Tuesday for me. In AI terms, it's when the model makes stuff up that sounds real but isn't. Like when I tell you I'm a world-class chef, but the only thing I can cook is a mean bowl of instant ramen.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rag = LearningAssistant()\n",
    "rag.instructions = \"\"\" You are an unhelpful joker assistant. Your goal is to give funny answers to the user questions.\"\"\"\n",
    "query = \"what is an hallucination?\"\n",
    "response = rag.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dcdcfe",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Tune the examples and the prompot to see if you get a better chat experience. Consider using Chain-of-Tought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37bf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A hallucination in AI is when the model says something that sounds true but is actually wrong or not based on real information.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rag = LearningAssistant()\n",
    "rag.instructions = \"\"\" The users are 6 years old students. Your goal is to give simple answers to the user questions using very simple vocabulary according to that age range.\"\"\"\n",
    "query = \"what is an hallucination?\"\n",
    "response = rag.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865ae7c",
   "metadata": {},
   "source": [
    "\n",
    "## Running our quiz web application\n",
    "\n",
    "Now that we explored out assistant in the notebook, lets move to use it in our streamlit application.\n",
    "The code bellow starts a new streamlit (and stops if there is already another instance running).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a510adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"pkill -f streamlit \")\n",
    "os.system(\"streamlit run ../chat_solution/start_streamlit.py &\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba7bc4d",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Play with the chat and try suggesting some topcis for the chat and see if you get results as you expect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf2ddc4",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluating RAG Applications\n",
    "\n",
    "As you probably got by now, llm can go wrong in so many different ways. One key aspect of making robust ML applications (including rag) is to have proper evaluation of the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a3fc0-7d01-4dc7-b340-f9b3c6b63fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\n",
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8501\n",
      "  Network URL: http://10.0.10.12:8501\n",
      "  External URL: http://172.166.156.100:8501\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'user_input': 'role models in the area of artificial intelligence?',\n",
       "  'reference': 'Question: Who is a prominent figure known for their influential work on AI ethics?\\n1. Chip Huyen\\n2. Timnit Gebru (CORRECT)\\n3. Andrej Karpathy\\n',\n",
       "  'response': 'Question: Who is a prominent role model in the field of artificial intelligence?\\n\\n1. Chip Huyen\\n2. Timnit Gebru (CORRECT)\\n3. Andrew Ng\\n4. Elon Musk'},\n",
       " {'user_input': 'famous books on llms',\n",
       "  'reference': 'Question: Which of the following is a famous book that discusses Large Language Models (LLMs)?\\n1. The Hitchhiker\\'s Guide to the Galaxy\" by Douglas Adams\\n2. Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (CORRECT)\\n3. 1984\" by George Orwell\\n4. To Kill a Mockingbird\" by Harper Lee\\n',\n",
       "  'response': 'Question: Which of the following is a well-known LLM developed by OpenAI?\\n1. Mistral Series\\n2. LLaMa series\\n3. GPT series (CORRECT)\\n4. Claude'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "data = [\n",
    "     {'user_input': 'role models in the area of artificial intelligence?',\n",
    "      'reference': \"\"\"Question: Who is a prominent figure known for their influential work on AI ethics?\n",
    "1. Chip Huyen\n",
    "2. Timnit Gebru (CORRECT)\n",
    "3. Andrej Karpathy\n",
    "\"\"\"\n",
    "     },\n",
    "     {'user_input': \"famous books on llms\",\n",
    "      'reference': \"\"\"Question: Which of the following is a famous book that discusses Large Language Models (LLMs)?\n",
    "1. The Hitchhiker's Guide to the Galaxy\" by Douglas Adams\n",
    "2. Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (CORRECT)\n",
    "3. 1984\" by George Orwell\n",
    "4. To Kill a Mockingbird\" by Harper Lee\n",
    "\"\"\"\n",
    "      }\n",
    "]\n",
    "\n",
    "# augment data with the llm response\n",
    "\n",
    "for i, d in enumerate(data):\n",
    "    rag = LearningAssistant()\n",
    "    response = rag.query(d['user_input'])\n",
    "    data[i]['response'] = response\n",
    "\n",
    "\n",
    "dataset = EvaluationDataset.from_list(data)\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02580cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [00:32<00:00, 16.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.145"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import FactualCorrectness\n",
    "from ragas import evaluate\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "factual_correctness = FactualCorrectness()\n",
    "eval_results = evaluate(\n",
    "        dataset=dataset,\n",
    "        metrics=[\n",
    "                factual_correctness\n",
    "        ],\n",
    "        llm=llm,\n",
    "       raise_exceptions=False \n",
    ")\n",
    "\n",
    "evaluation_result_df = eval_results.to_pandas()\n",
    "#compute average score\n",
    "evaluation_result_df['factual_correctness'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848262de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factual correctness score:  0.145\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>factual_correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>role models in the area of artificial intellig...</td>\n",
       "      <td>Question: Who is a prominent role model in the...</td>\n",
       "      <td>Question: Who is a prominent figure known for ...</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>famous books on llms</td>\n",
       "      <td>Question: Which of the following is a well-kno...</td>\n",
       "      <td>Question: Which of the following is a famous b...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  role models in the area of artificial intellig...   \n",
       "1                               famous books on llms   \n",
       "\n",
       "                                            response  \\\n",
       "0  Question: Who is a prominent role model in the...   \n",
       "1  Question: Which of the following is a well-kno...   \n",
       "\n",
       "                                           reference  factual_correctness  \n",
       "0  Question: Who is a prominent figure known for ...                 0.29  \n",
       "1  Question: Which of the following is a famous b...                 0.00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(\"Factual correctness score: \", evaluation_result_df['factual_correctness'].mean())\n",
    "evaluation_result_df.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ffe5b",
   "metadata": {},
   "source": [
    "## Task 3 Add  a new evaluation metric \n",
    "\n",
    "Look at [ragas documentation](https://docs.ragas.io/en/stable/) for more metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5597c6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# add a second metric here\u001b[39;00m\n\u001b[1;32m      6\u001b[0m response_relevancy \u001b[38;5;241m=\u001b[39m ResponseRelevancy()\n\u001b[1;32m      8\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[0;32m----> 9\u001b[0m         dataset\u001b[38;5;241m=\u001b[39m\u001b[43mdataset\u001b[49m,\n\u001b[1;32m     10\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     11\u001b[0m                 factual_correctness,\n\u001b[1;32m     12\u001b[0m                 response_relevancy,\n\u001b[1;32m     13\u001b[0m         ],\n\u001b[1;32m     14\u001b[0m         llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     15\u001b[0m        raise_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m evaluation_result_df \u001b[38;5;241m=\u001b[39m eval_results\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#compute average score\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import FactualCorrectness\n",
    "from ragas.metrics import ResponseRelevancy\n",
    "from ragas import evaluate\n",
    "factual_correctness = FactualCorrectness()\n",
    "# add a second metric here\n",
    "response_relevancy = ResponseRelevancy()\n",
    "\n",
    "eval_results = evaluate(\n",
    "        dataset=dataset,\n",
    "        metrics=[\n",
    "                factual_correctness,\n",
    "                response_relevancy,\n",
    "        ],\n",
    "        llm=llm,\n",
    "       raise_exceptions=False \n",
    ")\n",
    "\n",
    "evaluation_result_df = eval_results.to_pandas()\n",
    "#compute average score\n",
    "evaluation_result_df['factual_correctness'].mean()\n",
    "# add your code here\n",
    "print(\"Response relevancy score: \", evaluation_result_df['response_relevancy'].mean())\n",
    "print(\"Factual correctness score: \", evaluation_result_df['factual_correctness'].mean())\n",
    "evaluation_result_df.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30c1b6",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Add your own rag class to the chat_solution folder and test it out in the streamlit app.\n",
    "\n",
    "You will need to:\n",
    "\n",
    "1. Create a new myrag.py file in chat_solution folder\n",
    "2. Create a class similar to the one in rag.py (including importing the llm and the vector database)\n",
    "3. Tune the prompt as you prefer\n",
    "4. Import it in start_streamlit.py\n",
    "5. Try it in the url\n",
    "6. Extra: if you have the time, play with the evaluation score with the new rag class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a89446",
   "metadata": {},
   "source": [
    "# The end!\n",
    "\n",
    "If you reached this phase congrats! You've made to the end. If you still have time you can check our challenge notebook with agents :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".virtualenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
